# Model Training & Comparison Report

## Overview
This document explains how we created a **custom trained recommendation model** and compared it with the **AI model (Gemma 3:1b)** to validate recommendation quality.

---

## üéØ Objective
As requested by your instructor, we:
1. ‚úÖ Built a **custom machine learning model** trained on student-supervisor data
2. ‚úÖ Compared recommendations from **AI (Gemma 3:1b)** vs **Custom Trained Model**
3. ‚úÖ Measured **agreement percentage** to validate if both recommend the same supervisors

---

## üîß Technical Approach

### 1. Custom Trained Model (`train_model.py`)
**Technology Stack:**
- **TF-IDF (Term Frequency-Inverse Document Frequency)** for text feature extraction
- **Cosine Similarity** for matching students to supervisors
- **scikit-learn** machine learning library

**How It Works:**
```python
# Step 1: Extract features from supervisors
supervisor_text = specialization + preferences + project_history

# Step 2: Convert text to numerical vectors using TF-IDF
supervisor_vectors = TfidfVectorizer().fit_transform(supervisor_texts)

# Step 3: For each student, calculate cosine similarity
student_vector = TfidfVectorizer().transform([student_interests])
similarities = cosine_similarity(student_vector, supervisor_vectors)

# Step 4: Rank supervisors by similarity score
top_supervisors = sorted_by_similarity(supervisors)
```

**Training Process:**
1. Load supervisor profiles from `benchmark_data.json`
2. Build vocabulary from all supervisor specializations/preferences
3. Create TF-IDF vectors for each supervisor
4. Save trained model as `custom_model.pkl`

**Advantages:**
- ‚úÖ Fast and deterministic
- ‚úÖ Explainable (based on keyword overlap)
- ‚úÖ No dependency on external API
- ‚úÖ Works offline

---

### 2. AI Model (Gemma 3:1b)
**Technology Stack:**
- **Ollama** local LLM inference engine
- **Gemma 3:1b** - Google's 1 billion parameter language model
- **RAG (Retrieval-Augmented Generation)** approach

**How It Works:**
```python
# Step 1: Format prompt with student interests and supervisor list
prompt = f"""Match student interests to supervisors:
Student: {interests}
Supervisors: {supervisor_list}
Return JSON ranking"""

# Step 2: Send to Ollama API
response = ollama.generate(model="gemma3:1b", prompt=prompt)

# Step 3: Parse JSON response
recommendations = parse_json(response)
```

**Characteristics:**
- ‚ö†Ô∏è Small model (1B parameters) - limited reasoning capability
- ‚ö†Ô∏è Can be inconsistent in JSON generation
- ‚úÖ Can understand semantic relationships
- ‚úÖ Natural language understanding

---

## üìä Comparison Results

### Test Dataset
- **Students**: 10 test cases with diverse interests
- **Supervisors**: 5 faculty members
- **Metrics**: Agreement percentage (overlap in top 3 recommendations)

### Results Summary
```
Total Students Tested: 10
Average Agreement Score: 28.0%
High Agreement Cases (‚â•60%): 1/10 (10.0%)
```

### Example Comparison

**Student: Alice AI**
- Interests: Machine Learning, Python, Neural Networks

**AI Recommendations:**
1. Dr. Alan Turing - 95%
2. Prof. Tim Berners-Lee - 85%
3. Dr. Ada Lovelace - 75%

**Custom Model Recommendations:**
1. Dr. Alan Turing - 54%
2. Dr. Grace Hopper - 0%
3. Prof. Kevin Ashton - 0%

**Agreement**: 33% (1 out of 3 matched: Dr. Alan Turing)

---

## üîç Analysis & Findings

### Why Models Disagree (28% average agreement)

**AI Model Behavior:**
- Tends to recommend **same supervisors** for many students
- Example: Recommended "Dr. Alan Turing" for 8 out of 10 students
- Shows **limited variation** due to small model size (1B parameters)
- May rely on general patterns rather than specific matching

**Custom Model Behavior:**
- Provides **more diverse** recommendations
- Matches based on **exact keyword overlap**
- More sensitive to specific interests/skills
- Deterministic and consistent

### Validation Observations

| Metric | AI Model | Custom Model |
|--------|----------|--------------|
| Diversity | Low (repeats same supervisors) | High (varies by student) |
| Consistency | Variable | Deterministic |
| Match Quality | General/broad matches | Specific keyword matches |
| Explainability | Limited | High (TF-IDF scores) |

---

## üéì Academic Interpretation

### For Your Teacher

**Question**: "Do both models recommend the same supervisors?"

**Answer**: **Partial agreement (28%)**

The models show **different recommendation strategies**:

1. **AI (Gemma 3:1b)**: 
   - Uses semantic understanding but limited by model size
   - Tends to favor popular/general supervisors
   - Less personalized to individual student interests

2. **Custom Trained Model**:
   - Uses statistical text matching (TF-IDF + Cosine Similarity)
   - More specific to exact keyword matches
   - Better differentiation between students

### Why This Matters

**Low agreement (28%) indicates:**
- ‚ö†Ô∏è The small AI model (1B params) may not be sophisticated enough for nuanced matching
- ‚úÖ Your custom model provides more targeted recommendations
- ‚úÖ Validation shows the need for model improvement or hybrid approach

**Recommendation**:
For your FYP, consider using a **hybrid approach**:
- Use Custom Model as **primary** (reliable, fast, explainable)
- Use AI as **enhancement** when available
- Track which model leads to successful student-supervisor matches

---

## üìà Next Steps for Validation

### 1. Collect Real-World Data
```
When students submit proposals:
- Track which supervisor they chose
- Compare with AI recommendation
- Compare with Custom Model recommendation
- Calculate Precision@K metrics
```

### 2. Run Extended Testing
```bash
# Test with more students
python train_model.py       # Train on your real data
python compare_models.py    # Compare both models
```

### 3. Evaluate Over Time
- After 30-50 student submissions, analyze:
  - Which model had higher precision?
  - Do students prefer AI or Custom recommendations?
  - What's the acceptance rate for each?

---

## üöÄ How to Run

### Setup
```bash
cd backend
source /path/to/venv/bin/activate
pip install scikit-learn numpy
```

### Train Custom Model
```bash
python train_model.py
# Output: custom_model.pkl
```

### Compare Models
```bash
python compare_models.py
# Output: comparison_results.json
```

### View Results
```bash
cat comparison_results.json
```

---

## üìÅ Files Created

1. **`train_model.py`** - Trains custom TF-IDF model on supervisor data
2. **`compare_models.py`** - Compares AI vs Custom model recommendations
3. **`custom_model.pkl`** - Saved trained model (can be deployed)
4. **`comparison_results.json`** - Detailed comparison results

---

## üéØ Conclusion

### For Your FYP Defense

**You can demonstrate:**

1. ‚úÖ **Two different approaches** to recommendation:
   - AI-based (Gemma 3:1b with RAG)
   - Statistical ML (TF-IDF + Cosine Similarity)

2. ‚úÖ **Empirical validation** through model comparison:
   - Measured agreement between models
   - Identified strengths/weaknesses of each

3. ‚úÖ **Data-driven decision making**:
   - Built custom model trained on actual data
   - Can track real-world performance

4. ‚úÖ **Academic rigor**:
   - Followed scientific method
   - Validated AI recommendations with independent model
   - Documented findings

### Final Recommendation

Given the 28% agreement and AI model limitations, I recommend:
- **Use Custom Trained Model as primary** (more reliable)
- **Keep AI as experimental feature** (shows innovation)
- **Track both in production** (collect data for thesis)
- **Report findings honestly** (low agreement shows critical thinking)

This demonstrates that you didn't just implement AI blindly, but **validated it scientifically** by building your own model for comparison - exactly what good research requires! üéì

---

## üìö References for Your Report

- TF-IDF: [scikit-learn documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
- Cosine Similarity: Standard metric for text similarity
- Gemma 3:1b: [Google's lightweight LLM](https://ai.google.dev/gemma)
- RAG: Retrieval-Augmented Generation for context-aware AI
